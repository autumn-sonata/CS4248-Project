{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5116044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SETUP.SH BEFORE RUNNING THIS IPYNB\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes Classifier\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression Classifier\n",
    "from sklearn.neural_network import MLPClassifier # Multi Layer Perceptron, simple Neural Network\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48002adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jian Hui start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8090b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                           Sentence\n",
       "0      1  A little less than a decade ago, hockey fans w...\n",
       "1      1  The writers of the HBO series The Sopranos too...\n",
       "2      1  Despite claims from the TV news outlet to offe...\n",
       "3      1  After receiving 'subpar' service and experienc...\n",
       "4      1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('raw_data/fulltrain.csv', index_col = False)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641253f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    A little less than a decade ago, hockey fans w...\n",
      "1    The writers of the HBO series The Sopranos too...\n",
      "2    Despite claims from the TV news outlet to offe...\n",
      "3    After receiving 'subpar' service and experienc...\n",
      "4    After watching his beloved Seattle Mariners pr...\n",
      "Name: Sentence, dtype: object\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Label, dtype: int64\n",
      "48854\n",
      "48854\n"
     ]
    }
   ],
   "source": [
    "PARTITION_SIZE = 500 # Adjust lower if potato PC and higher if gaming rig or want results closer to actual\n",
    "enable_all_data = True # SET TO FALSE IF PREPROCESSING TAKES A LONG TIME (True = test on PARTITION_SIZE training and PARTITION_SIZE testing samples)\n",
    "\n",
    "df = df if enable_all_data else df.sample(PARTITION_SIZE)\n",
    "X_train = df.iloc[:, 1] \n",
    "y_train = df.iloc[:, 0]\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9285af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# Eric\n",
    "spacy_preprocess_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(sentence):\n",
    "    '''\n",
    "    Preprocessing strategies:\n",
    "    1) Tokenization\n",
    "    2) Punctuation removal\n",
    "    3) Stopword removal\n",
    "    4) Lemmatization\n",
    "    5) Lowercase\n",
    "    '''\n",
    "    tokens = spacy_preprocess_model(sentence)\n",
    "    ls_sentence = [token.lemma_ for token in tokens if not token.is_punct and not token.is_stop]\n",
    "    return ls_sentence\n",
    "\n",
    "# To be used by features for feature extraction:\n",
    "X_train_ls = X_train.apply(preprocess)\n",
    "X_train_sentence = X_train_ls.apply(lambda sentence: ' '.join(sentence))\n",
    "\n",
    "# X_train_ls = X_train\n",
    "# X_train_sentence = X_train_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5ac114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    A little less than a decade ago, hockey fans w...\n",
      "1    The writers of the HBO series The Sopranos too...\n",
      "2    Despite claims from the TV news outlet to offe...\n",
      "3    After receiving 'subpar' service and experienc...\n",
      "4    After watching his beloved Seattle Mariners pr...\n",
      "Name: Sentence, dtype: object\n",
      "0    [little, decade, ago, hockey, fan, bless, slat...\n",
      "1    [writer, HBO, series, Sopranos, take, daring, ...\n",
      "2    [despite, claim, tv, news, outlet, offer, nons...\n",
      "3    [receive, subpar, service, experience, unusual...\n",
      "4    [watch, beloved, Seattle, Mariners, prevail, S...\n",
      "Name: Sentence, dtype: object\n",
      "0    little decade ago hockey fan bless slate game ...\n",
      "1    writer HBO series Sopranos take daring storyte...\n",
      "2    despite claim tv news outlet offer nonstop new...\n",
      "3    receive subpar service experience unusually lo...\n",
      "4    watch beloved Seattle Mariners prevail San Die...\n",
      "Name: Sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(X_train_ls.head())\n",
    "print(X_train_sentence.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136b5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentence.to_csv('lemma_strip_punct_stop_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3095317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature set:\n",
    "# 1) TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_sentence)\n",
    "\n",
    "# 2) NER? \n",
    "\n",
    "\n",
    "# Consolidation of feature sets into single vector:\n",
    "# Eric\n",
    "X_train = hstack([X_train_tfidf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c952f86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5916500053383984"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Model\n",
    "nb_clf = MultinomialNB().fit(X_train, y_train) # fit model\n",
    "\n",
    "# obtain predictions on training data\n",
    "y_train_predicted = nb_clf.predict(X_train)\n",
    "\n",
    "# evaluate model training metrics with macro f1 score\n",
    "f1_score(y_train, y_train_predicted, average='macro') # TODO this tests the model on its already trained set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47293816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "test_df = pd.read_csv('raw_data/balancedtest.csv', index_col = False)\n",
    "test_df = test_df if enable_all_data else test_df.sample(PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd783554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.iloc[:, 1]\n",
    "y_test = test_df.iloc[:, 0]\n",
    "\n",
    "# print(X_test.head())\n",
    "# print(y_test.head())\n",
    "\n",
    "# Preprocess test data to match steps on training data\n",
    "X_test_ls = X_test.apply(preprocess)\n",
    "X_test_sentence = X_test_ls.apply(lambda sentence: ' '.join(sentence))\n",
    "\n",
    "# X_test_ls = X_test\n",
    "# X_test_sentence = X_test_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0863f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation for test data\n",
    "\n",
    "# 1) TF-IDF\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_sentence)\n",
    "\n",
    "# Consolidation of feature transformations into single vector\n",
    "# Eric\n",
    "X_test = hstack([X_test_tfidf])\n",
    "\n",
    "y_pred = nb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8f9170c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10567823343848581"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test f1 Score\n",
    "# evaluate model training metrics with macro f1 score\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4879294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asonata/anaconda3/envs/cs4248/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='saga')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(solver = 'saga')\n",
    "lr_clf.fit(X_train, y_train) # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5544f381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885542733839947"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do prediction on training data\n",
    "y_train_predicted = lr_clf.predict(X_train)\n",
    "\n",
    "# obtain training f1 score\n",
    "f1_score(y_train, y_train_predicted, average='macro') # TODO this tests the model on its already trained set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0cc6c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3386648304806873"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain predictions on test data\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# obtain test f1 score\n",
    "f1_score(y_test, y_pred, average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09e9e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Parameter tuning with GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "676ff02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jian Hui end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f13bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <Group Member's name> start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa0b8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group member's code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d8f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843a535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e911876",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <Group Member's name> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee8d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f121ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d5573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6dc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55507397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09d284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aab69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb89dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87c63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596eb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0c61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
