{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c7e43e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5116044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SETUP.SH BEFORE RUNNING THIS IPYNB\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes Classifier\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression Classifier\n",
    "from sklearn.neural_network import MLPClassifier # Multi Layer Perceptron, simple Neural Network\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import ADASYN, RandomOverSampler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81c927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 # seed random state for comparison, testing\n",
    "PARTITION_SIZE = 500 # Adjust lower if potato PC and higher if gaming rig or want results closer to actual\n",
    "enable_all_data = True # SET TO FALSE IF PREPROCESSING TAKES A LONG TIME (True = test on PARTITION_SIZE training and PARTITION_SIZE testing samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f33710",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48002adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jian Hui start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a8090b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  1  A little less than a decade ago, hockey fans w...\n",
       "1  1  The writers of the HBO series The Sopranos too...\n",
       "2  1  Despite claims from the TV news outlet to offe...\n",
       "3  1  After receiving 'subpar' service and experienc...\n",
       "4  1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('raw_data/fulltrain.csv', header=None, index_col = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "641253f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        A little less than a decade ago, hockey fans w...\n",
      "1        The writers of the HBO series The Sopranos too...\n",
      "2        Despite claims from the TV news outlet to offe...\n",
      "3        After receiving 'subpar' service and experienc...\n",
      "4        After watching his beloved Seattle Mariners pr...\n",
      "                               ...                        \n",
      "48849    The ruling Kuomintang (KMT) has claimed owners...\n",
      "48850    The Taipei city government has encouraged the ...\n",
      "48851    President Ma Ying-jeou said Friday that a park...\n",
      "48852    The families of the four people who were kille...\n",
      "48853    The Ministry of Finance will make public on Sa...\n",
      "Name: 1, Length: 48854, dtype: object\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "48849    4\n",
      "48850    4\n",
      "48851    4\n",
      "48852    4\n",
      "48853    4\n",
      "Name: 0, Length: 48854, dtype: int64\n",
      "48854\n",
      "48854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "3    17870\n",
       "1    14047\n",
       "4     9995\n",
       "2     6942\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df if enable_all_data else df.sample(n=PARTITION_SIZE, random_state=SEED)\n",
    "\n",
    "X_train = df.iloc[:, 1] \n",
    "y_train = df.iloc[:, 0]\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9285af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# Lemmatization, Case-folding (lowercase), Stopword removal, Punctuation removal\n",
    "# Eric\n",
    "personal_pronouns = [\"i\", \"me\", \"mine\", \"my\", \"myself\", \"our\", \"ours\", \"we\", \\\n",
    "                     \"their\", \"you\", \"your\", \"he\", \"she\", \"it\", \"its\", \"we\", \"they\", \"me\", \\\n",
    "                     \"him\", \"her\", \"us\", \"them\", \"his\", \"hers\", \"herself\", \\\n",
    "                        \"himself\", \"itself\", \"themselves\", \"ourselves\", \"yourself\", \"yourselves\"]\n",
    "spacy_preprocess_model = spacy.load(\"en_core_web_lg\")\n",
    "spacy_preprocess_model.Defaults.stop_words -= set(personal_pronouns)\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     '''\n",
    "#     Preprocessing strategies:\n",
    "#     1) Tokenization\n",
    "#     2) Punctuation removal\n",
    "#     3) Stopword removal\n",
    "#     4) Lemmatization\n",
    "#     5) Lowercase\n",
    "#     '''\n",
    "#     tokens = spacy_preprocess_model(sentence)\n",
    "#     ls_sentence = [token.lemma_.lower() for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"]) and not token.is_stop]\n",
    "#     return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5945f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# Case-folding (lowercase), Stopword removal, Punctuation removal\n",
    "\n",
    "def preprocess(sentence):\n",
    "    '''\n",
    "    Preprocessing strategies:\n",
    "    1) Tokenization\n",
    "    2) Punctuation removal\n",
    "    3) Stopword removal\n",
    "    4) Lowercase\n",
    "    '''\n",
    "    tokens = spacy_preprocess_model(sentence)\n",
    "    ls_sentence = [token.text.lower() for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"]) and not token.is_stop]\n",
    "    return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b12ef4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# Case-folding (lowercase), Punctuation removal\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     '''\n",
    "#     Preprocessing strategies:\n",
    "#     1) Tokenization\n",
    "#     2) Punctuation removal\n",
    "#     3) Lowercase\n",
    "#     '''\n",
    "#     tokens = spacy_preprocess_model(sentence)\n",
    "#     ls_sentence = [token.text.lower() for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"])]\n",
    "#     return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfc63bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# pos (TAG), Punctuation removal\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     '''\n",
    "#     Preprocessing strategies:\n",
    "#     1) Tokenization\n",
    "#     2) Punctuation removal\n",
    "#     3) POS tag\n",
    "#     '''\n",
    "#     tokens = spacy_preprocess_model(sentence)\n",
    "#     ls_sentence = [token.tag_ for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"])]\n",
    "#     return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96ab75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be used by features for feature extraction:\n",
    "# X_train_ls = X_train.apply(preprocess)\n",
    "# X_train_sentence = X_train_ls.apply(lambda sentence: ' '.join(sentence))\n",
    "\n",
    "# X_train_ls = X_train\n",
    "# X_train_sentence = X_train_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99544445",
   "metadata": {},
   "source": [
    "### Save and load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c5103f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_sentence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e62e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre-processed data\n",
    "# compression_opts = dict(method='zip', archive_name='strip_punct_stop_lower.csv')\n",
    "# X_train_sentence.to_csv('strip_punct_stop_lower.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f090821c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x396155360>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quickload pre-processed data\n",
    "# replace 'raw_data/*.csv' with .csv file containing preprocessed data\n",
    "X_train = pd.read_csv('strip_punct_stop_lower.csv', index_col=False).iloc[:, 0]\n",
    "X_train.head()\n",
    "\n",
    "# Reload constants if preprocessing cells are not executed\n",
    "personal_pronouns = [\"i\", \"me\", \"mine\", \"my\", \"myself\", \"our\", \"ours\", \"we\", \\\n",
    "                     \"their\", \"you\", \"your\", \"he\", \"she\", \"it\", \"its\", \"we\", \"they\", \"me\", \\\n",
    "                     \"him\", \"her\", \"us\", \"them\", \"his\", \"hers\", \"herself\", \\\n",
    "                        \"himself\", \"itself\", \"themselves\", \"ourselves\", \"yourself\", \"yourselves\"]\n",
    "spacy_model = spacy.load(\"en_core_web_lg\")\n",
    "spacy_model.Defaults.stop_words -= set(personal_pronouns)\n",
    "spacy_model.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b2070",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b08f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ca07b",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3095317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature set:\n",
    "# 1) TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fdd8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Sentiment analysis\n",
    "def sentiment_analysis(sentence):\n",
    "    doc = spacy_model(sentence)\n",
    "    return doc._.blob.polarity ** 2 # square to flag extreme polarity text\n",
    "\n",
    "X_train_sent_ana = X_train.apply(sentiment_analysis).to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "261ff629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Subjectivity analysis \n",
    "def subjectivity_analysis(sentence):\n",
    "    doc = spacy_model(sentence)\n",
    "    return doc._.blob.subjectivity\n",
    "\n",
    "X_train_subj_ana = X_train.apply(subjectivity_analysis).to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50493e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Number of exclamation and question marks (pre-analyze first)\n",
    "def count_exclamation_marks(sentence):\n",
    "    return len(re.findall(r'\\!', sentence))\n",
    "\n",
    "def count_question_marks(sentence):\n",
    "    return len(re.findall(r'\\?', sentence))\n",
    "\n",
    "X_train_count_ex = X_train.apply(count_exclamation_marks).to_numpy().reshape(-1, 1)\n",
    "X_train_count_qn = X_train.apply(count_question_marks).to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "378359e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Summation of word2vec on top k words tf-idf per sentence\n",
    "k = 5\n",
    "tfidf_vectorizer_word2vec = TfidfVectorizer(sublinear_tf=True)\n",
    "tfidf_word2vec = tfidf_vectorizer_word2vec.fit_transform(X_train)\n",
    "non_zero_counts = tfidf_word2vec.getnnz(axis=1)\n",
    "\n",
    "def word2vec_select_k_best_sum(matrix):\n",
    "    res = None\n",
    "    for row in range(matrix.shape[0]):\n",
    "        # Get indexes of words\n",
    "        num_words = min(k, non_zero_counts[row])\n",
    "        row_val_idxs = matrix[row].indices\n",
    "        top_k_idx = np.argpartition(matrix[row].data, -num_words)[-num_words:]\n",
    "        indexes = row_val_idxs[top_k_idx]\n",
    "        \n",
    "        # Get words\n",
    "        words = tfidf_vectorizer_word2vec.get_feature_names_out()[indexes]\n",
    "\n",
    "        # Get summation of word vectors\n",
    "        summation_vector = np.sum(np.array([spacy_model(word).vector for word in words]), axis=0)\n",
    "        if res is None:\n",
    "            res = np.zeros([matrix.shape[0], summation_vector.shape[0]])\n",
    "        res[row] = summation_vector\n",
    "\n",
    "    return res\n",
    "\n",
    "X_train_word2vec = word2vec_select_k_best_sum(tfidf_word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f48d5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Number of personal pronouns\n",
    "def count_personal_pronouns(sentence):\n",
    "    counter = 0\n",
    "    sentence_ls = sentence.split()\n",
    "    for token in sentence_ls:\n",
    "        if token in personal_pronouns:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "X_train_count_pp = X_train.apply(count_personal_pronouns).to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0156ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) \n",
    "\n",
    "# Model: LSTM? Random forest, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff0f8f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39083, 5991236)\n",
      "(39083, 1)\n",
      "(39083, 1)\n",
      "(39083, 1)\n",
      "(39083, 1)\n",
      "(39083, 300)\n",
      "(39083, 1)\n"
     ]
    }
   ],
   "source": [
    "# Consolidation of feature sets into single vector:\n",
    "# Eric\n",
    "# print(X_train_tfidf.shape)\n",
    "# print(X_train_sent_ana.shape)\n",
    "# print(X_train_subj_ana.shape)\n",
    "# print(X_train_count_ex.shape)\n",
    "# print(X_train_count_qn.shape)\n",
    "# print(X_train_word2vec.shape)\n",
    "# print(X_train_count_pp.shape)\n",
    "X_train = hstack([X_train_tfidf, X_train_sent_ana, X_train_subj_ana, X_train_count_ex, X_train_count_qn, X_train_word2vec, X_train_count_pp])\n",
    "# X_train = hstack([X_train_tfidf, X_train_count_ex, X_train_count_qn, X_train_count_pp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce5211",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7b26eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=SEED)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a609fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADASYN\n",
    "ada = ADASYN(random_state=SEED)\n",
    "X_train, y_train = ada.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4eaf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83844d2d",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f2107",
   "metadata": {},
   "source": [
    "### Naive Bayes Model [MultinomialNB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c952f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MultinomialNB().fit(X_train, y_train) # fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89589e",
   "metadata": {},
   "source": [
    "### Logistic Regression Model [LogisticRegression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4879294",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = 'saga', max_iter=1000).fit(X_train, y_train) # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9bd3c",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering on X_val\n",
    "val_tfidf_word2vec = tfidf_vectorizer_word2vec.transform(X_val)\n",
    "non_zero_counts = val_tfidf_word2vec.getnnz(axis=1)\n",
    "\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_val_sent_ana = X_val.apply(sentiment_analysis).to_numpy().reshape(-1, 1)\n",
    "X_val_subj_ana = X_val.apply(subjectivity_analysis).to_numpy().reshape(-1, 1)\n",
    "X_val_count_ex = X_val.apply(count_exclamation_marks).to_numpy().reshape(-1, 1)\n",
    "X_val_count_qn = X_val.apply(count_question_marks).to_numpy().reshape(-1, 1)\n",
    "X_val_word2vec = word2vec_select_k_best_sum(val_tfidf_word2vec)\n",
    "X_val_count_pp = X_val.apply(count_personal_pronouns).to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation of feature sets:\n",
    "X_val = hstack([X_val_tfidf, X_val_sent_ana, X_val_subj_ana, X_val_count_ex, X_val_count_qn, X_val_word2vec, X_val_count_pp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0b24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1518813832210427"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain predictions on validation data\n",
    "y_val_predicted = model.predict(X_val)\n",
    "\n",
    "# evaluate model training metrics with macro f1 score\n",
    "f1_score(y_val, y_val_predicted, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8997e6",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47293816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "test_df = pd.read_csv('raw_data/balancedtest.csv', index_col = False)\n",
    "test_df = test_df if enable_all_data else test_df.sample(PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd783554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.iloc[:, 1]\n",
    "y_test = test_df.iloc[:, 0]\n",
    "\n",
    "# print(X_test.head())\n",
    "# print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bcfb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[231], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess test data to match steps on training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_test_ls \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m X_test_sentence \u001b[38;5;241m=\u001b[39m X_test_ls\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m sentence: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence))\n\u001b[1;32m      5\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test_sentence\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[205], line 12\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(sentence):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Preprocessing strategies:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    1) Tokenization\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    4) Lowercase\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_preprocess_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     ls_sentence \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (token\u001b[38;5;241m.\u001b[39mis_punct \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ls_sentence\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1018\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/language.py:1128\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/language.py:1120\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1118\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/tokenizer.pyx:151\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/tokenizer.pyx:187\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/tokenizer.pyx:391\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/tokenizer.pyx:485\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/vocab.pyx:165\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/vocab.pyx:202\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248/lib/python3.10/site-packages/spacy/lang/lex_attrs.py:144\u001b[0m, in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    140\u001b[0m             shape\u001b[38;5;241m.\u001b[39mappend(shape_char)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(shape)\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m string\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprefix\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocess test data to match steps on training data\n",
    "X_test_ls = X_test.apply(preprocess)\n",
    "X_test_sentence = X_test_ls.apply(lambda sentence: ' '.join(sentence))\n",
    "\n",
    "X_test = X_test_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863f80a",
   "metadata": {},
   "source": [
    "### Feature Engineering (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0afa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_word2vec = tfidf_vectorizer_word2vec.transform(X_test)\n",
    "non_zero_counts = test_tfidf_word2vec.getnnz(axis=1)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_test_sent_ana = X_test.apply(sentiment_analysis).to_numpy().reshape(-1, 1)\n",
    "X_test_subj_ana = X_test.apply(subjectivity_analysis).to_numpy().reshape(-1, 1)\n",
    "X_test_count_ex = X_test.apply(count_exclamation_marks).to_numpy().reshape(-1, 1)\n",
    "X_test_count_qn = X_test.apply(count_question_marks).to_numpy().reshape(-1, 1)\n",
    "X_test_word2vec = word2vec_select_k_best_sum(test_tfidf_word2vec)\n",
    "X_test_count_pp = X_test.apply(count_personal_pronouns).to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation of feature transformations into single vector\n",
    "# Eric\n",
    "X_test = hstack([X_test_tfidf, X_test_sent_ana, X_test_subj_ana, X_test_count_ex, X_test_count_qn, X_test_word2vec, X_test_count_pp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate model training metrics with macro f1 score\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4ce93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction on training data\n",
    "y_train_predicted = model.predict(X_train)\n",
    "\n",
    "# obtain training f1 score\n",
    "f1_score(y_train, y_train_predicted, average='macro') # TODO this tests the model on its already trained set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# obtain test f1 score\n",
    "f1_score(y_test, y_pred, average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Parameter tuning with GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ff02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jian Hui end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <Group Member's name> start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group member's code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d8f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843a535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e911876",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <Group Member's name> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee8d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f121ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d5573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6dc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55507397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09d284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aab69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb89dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87c63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596eb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0c61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
