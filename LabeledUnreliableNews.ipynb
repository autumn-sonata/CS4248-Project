{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c7e43e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5116044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SETUP.SH BEFORE RUNNING THIS IPYNB\n",
    "# REQUIREMENTS FOR SETUP.SH:\n",
    "# python 3.11.8\n",
    "# pip 23.3.1\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes Classifier\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression Classifier\n",
    "from sklearn.neural_network import MLPClassifier # Multi Layer Perceptron, simple Neural Network\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import ADASYN, RandomOverSampler\n",
    "from scipy.sparse import hstack, csr_matrix, save_npz, load_npz\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, f_regression, mutual_info_classif\n",
    "import nltk\n",
    "from readability import Readability\n",
    "import text2emotion as te\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b81c927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 # seed random state for comparison, testing\n",
    "PARTITION_SIZE = 500 # Adjust lower if potato PC and higher if gaming rig or want results closer to actual\n",
    "enable_all_data = True # SET TO FALSE IF PREPROCESSING TAKES A LONG TIME (True = test on PARTITION_SIZE training and PARTITION_SIZE testing samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f33710",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48002adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jian Hui start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a8090b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  1  A little less than a decade ago, hockey fans w...\n",
       "1  1  The writers of the HBO series The Sopranos too...\n",
       "2  1  Despite claims from the TV news outlet to offe...\n",
       "3  1  After receiving 'subpar' service and experienc...\n",
       "4  1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('raw_data/fulltrain.csv', header=None, index_col = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "641253f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        A little less than a decade ago, hockey fans w...\n",
      "1        The writers of the HBO series The Sopranos too...\n",
      "2        Despite claims from the TV news outlet to offe...\n",
      "3        After receiving 'subpar' service and experienc...\n",
      "4        After watching his beloved Seattle Mariners pr...\n",
      "                               ...                        \n",
      "48849    The ruling Kuomintang (KMT) has claimed owners...\n",
      "48850    The Taipei city government has encouraged the ...\n",
      "48851    President Ma Ying-jeou said Friday that a park...\n",
      "48852    The families of the four people who were kille...\n",
      "48853    The Ministry of Finance will make public on Sa...\n",
      "Name: 1, Length: 48854, dtype: object\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "48849    4\n",
      "48850    4\n",
      "48851    4\n",
      "48852    4\n",
      "48853    4\n",
      "Name: 0, Length: 48854, dtype: int64\n",
      "48854\n",
      "48854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "3    17870\n",
       "1    14047\n",
       "4     9995\n",
       "2     6942\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df if enable_all_data else df.sample(n=PARTITION_SIZE, random_state=SEED)\n",
    "\n",
    "X_train = df.iloc[:, 1] \n",
    "y_train = df.iloc[:, 0]\n",
    "X_train_unprocessed = X_train\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9285af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# Lemmatization, Case-folding (lowercase), Stopword removal, Punctuation removal\n",
    "# Eric\n",
    "personal_pronouns = [\"i\", \"me\", \"mine\", \"my\", \"myself\", \"our\", \"ours\", \"we\", \\\n",
    "                     \"their\", \"you\", \"your\", \"he\", \"she\", \"it\", \"its\", \"we\", \"they\", \"me\", \\\n",
    "                     \"him\", \"her\", \"us\", \"them\", \"his\", \"hers\", \"herself\", \\\n",
    "                        \"himself\", \"itself\", \"themselves\", \"ourselves\", \"yourself\", \"yourselves\"]\n",
    "spacy_preprocess_model = spacy.load(\"en_core_web_lg\")\n",
    "spacy_preprocess_model.Defaults.stop_words -= set(personal_pronouns)\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     '''\n",
    "#     Preprocessing strategies:\n",
    "#     1) Tokenization\n",
    "#     2) Punctuation removal\n",
    "#     3) Stopword removal\n",
    "#     4) Lemmatization\n",
    "#     5) Lowercase\n",
    "#     '''\n",
    "#     tokens = spacy_preprocess_model(sentence)\n",
    "#     ls_sentence = [token.lemma_.lower() for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"]) and not token.is_stop]\n",
    "#     return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5945f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# Case-folding (lowercase), Stopword removal, Punctuation removal\n",
    "\n",
    "def preprocess(sentence):\n",
    "    '''\n",
    "    Preprocessing strategies:\n",
    "    1) Tokenization\n",
    "    2) Punctuation removal\n",
    "    3) Stopword removal\n",
    "    4) Lowercase\n",
    "    '''\n",
    "    tokens = spacy_preprocess_model(sentence)\n",
    "    ls_sentence = [token.text.lower() for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"]) and not token.is_stop]\n",
    "    return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b12ef4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# Case-folding (lowercase), Punctuation removal\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     '''\n",
    "#     Preprocessing strategies:\n",
    "#     1) Tokenization\n",
    "#     2) Punctuation removal\n",
    "#     3) Lowercase\n",
    "#     '''\n",
    "#     tokens = spacy_preprocess_model(sentence)\n",
    "#     ls_sentence = [token.text.lower() for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"])]\n",
    "#     return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dfc63bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data: tokenize the text for NLP Machine Learning\n",
    "# pos (TAG), Punctuation removal\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     '''\n",
    "#     Preprocessing strategies:\n",
    "#     1) Tokenization\n",
    "#     2) Punctuation removal\n",
    "#     3) POS tag\n",
    "#     '''\n",
    "#     tokens = spacy_preprocess_model(sentence)\n",
    "#     ls_sentence = [token.tag_ for token in tokens if not (token.is_punct and token not in [\"!\", \"?\"])]\n",
    "#     return ls_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "96ab75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be used by features for feature extraction:\n",
    "# X_train_ls = X_train.apply(preprocess)\n",
    "# X_train_sentence = X_train_ls.apply(lambda sentence: ' '.join(sentence))\n",
    "\n",
    "# X_train_ls = X_train\n",
    "# X_train_sentence = X_train_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99544445",
   "metadata": {},
   "source": [
    "### Save and load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c5103f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_sentence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e62e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre-processed data\n",
    "# compression_opts = dict(method='zip', archive_name='strip_punct_stop_lower.csv')\n",
    "# X_train_sentence.to_csv('strip_punct_stop_lower.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f090821c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x3bf14a650>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quickload pre-processed data\n",
    "# replace 'raw_data/*.csv' with .csv file containing preprocessed data\n",
    "X_train = pd.read_csv('strip_punct_stop_lower.csv', index_col=False).iloc[:, 0]\n",
    "X_train.head()\n",
    "\n",
    "# Reload constants if preprocessing cells are not executed\n",
    "personal_pronouns = [\"i\", \"me\", \"mine\", \"my\", \"myself\", \"our\", \"ours\", \"we\", \\\n",
    "                     \"their\", \"you\", \"your\", \"he\", \"she\", \"it\", \"its\", \"we\", \"they\", \"me\", \\\n",
    "                     \"him\", \"her\", \"us\", \"them\", \"his\", \"hers\", \"herself\", \\\n",
    "                        \"himself\", \"itself\", \"themselves\", \"ourselves\", \"yourself\", \"yourselves\"]\n",
    "spacy_model = spacy.load(\"en_core_web_lg\")\n",
    "spacy_model.Defaults.stop_words -= set(personal_pronouns)\n",
    "spacy_model.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b2070",
   "metadata": {},
   "source": [
    "### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b08f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unprocessed, X_val_unprocessed, _, _ = train_test_split(X_train_unprocessed, y_train, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04d7d4",
   "metadata": {},
   "source": [
    "### Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6bd00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_analysis_bar(X, y, feature_name):\n",
    "    unique_y = np.unique(y)\n",
    "    fig, axs = plt.subplots(len(unique_y), 1, figsize=(8, len(unique_y) * 4))\n",
    "    for i in unique_y:\n",
    "        indices = np.where(y == i)[0]\n",
    "        counts_i = X[indices]\n",
    "        unique_vals, counts = np.unique(counts_i, return_counts=True)\n",
    "        axs[i-1].bar(unique_vals, counts)\n",
    "        axs[i-1].set_xlabel(f'{feature_name}')\n",
    "        axs[i-1].set_ylabel(f'Frequency of {feature_name}')\n",
    "        axs[i-1].set_title(f'Distribution of {feature_name} for class {i}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{feature_name}_analysis_plot.png')\n",
    "\n",
    "def count_analysis_scatter(X, y, feature_name):\n",
    "    unique_y = np.unique(y)\n",
    "    fig, axs = plt.subplots(len(unique_y), 1, figsize=(8, len(unique_y) * 4))\n",
    "    for i in unique_y:\n",
    "        indices = np.where(y == i)[0]\n",
    "        counts_i = X[indices]\n",
    "        unique_vals, counts = np.unique(counts_i, return_counts=True)\n",
    "        axs[i-1].scatter(unique_vals, counts)\n",
    "        axs[i-1].set_xlabel(f'{feature_name}')\n",
    "        axs[i-1].set_ylabel(f'Frequency of {feature_name}')\n",
    "        axs[i-1].set_title(f'Distribution of {feature_name} for class {i}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{feature_name}_analysis_plot.png')\n",
    "\n",
    "def count_analysis_boxplot(X, y, feature_name):\n",
    "    unique_y = np.unique(y)\n",
    "    fig, axs = plt.subplots(len(unique_y), 1, figsize=(8, len(unique_y) * 4))\n",
    "    for i in unique_y:\n",
    "        indices = np.where(y == i)[0]\n",
    "        counts_i = X[indices]\n",
    "        unique_vals = np.unique(counts_i)\n",
    "        axs[i-1].boxplot(unique_vals)\n",
    "        axs[i-1].set_title(f'Distribution of {feature_name} for class {i}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{feature_name}_analysis_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ca07b",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c3095317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature set:\n",
    "# 1) TF-IDF and TF\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), sublinear_tf=True)\n",
    "tf_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_train_tf = tf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "378359e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Summation of word2vec on top k words tf-idf per sentence\n",
    "k = 5\n",
    "tfidf_vectorizer_word2vec = TfidfVectorizer(sublinear_tf=True)\n",
    "tfidf_word2vec = tfidf_vectorizer_word2vec.fit_transform(X_train)\n",
    "non_zero_counts = tfidf_word2vec.getnnz(axis=1)\n",
    "\n",
    "def word2vec_select_k_best_sum(matrix, non_zero_counts):\n",
    "    res = None\n",
    "    for row in range(matrix.shape[0]):\n",
    "        # Get indexes of words\n",
    "        num_words = min(k, non_zero_counts[row])\n",
    "        row_val_idxs = matrix[row].indices\n",
    "        top_k_idx = np.argpartition(matrix[row].data, -num_words)[-num_words:]\n",
    "        indexes = row_val_idxs[top_k_idx]\n",
    "        \n",
    "        # Get words\n",
    "        words = tfidf_vectorizer_word2vec.get_feature_names_out()[indexes]\n",
    "\n",
    "        # Get summation of word vectors\n",
    "        summation_vector = np.sum(np.array([spacy_model(word).vector for word in words]), axis=0)\n",
    "        if res is None:\n",
    "            res = np.zeros([matrix.shape[0], summation_vector.shape[0]])\n",
    "        res[row] = summation_vector\n",
    "\n",
    "    return res\n",
    "\n",
    "X_train_word2vec = word2vec_select_k_best_sum(tfidf_word2vec, non_zero_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f48d5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Number of personal pronouns\n",
    "def count_personal_pronouns(sentence):\n",
    "    counter = 0\n",
    "    sentence_ls = sentence.split()\n",
    "    for token in sentence_ls:\n",
    "        if token in personal_pronouns:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "X_train_count_pp = X_train.apply(count_personal_pronouns).to_numpy().reshape(-1, 1)\n",
    "# count_analysis_scatter(X_train_count_pp, y_train, \"personal_pronouns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7bdb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Count number of words\n",
    "def count_words(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "X_train_count_words = X_train_unprocessed.apply(count_words).to_numpy().reshape(-1, 1)\n",
    "# count_analysis_scatter(X_train_count_words, y_train, \"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc30e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Number of upper case letters\n",
    "def count_uppercase(text):\n",
    "    return len(re.findall(r\"[A-Z]\", text))\n",
    "\n",
    "X_train_count_uppercase = X_train_unprocessed.apply(count_uppercase).to_numpy().reshape(-1, 1)\n",
    "# count_analysis_scatter(X_train_count_uppercase, y_train, \"uppercase_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a7698f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Flesch Kincaid reading ease\n",
    "def flesch_kincaid(sentence):\n",
    "    if len(sentence.split()) <= 110:\n",
    "        return 65 # Standard average value for Flesch Kincaid reading ease score \n",
    "    return Readability(sentence).flesch().score\n",
    "\n",
    "X_train_flesch = X_train_unprocessed.apply(flesch_kincaid).to_numpy().reshape(-1, 1)\n",
    "# count_analysis_boxplot(X_train_flesch, y_train, \"flesch_kincaid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0156ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Verb counting\n",
    "from collections import Counter\n",
    "def count_verbs(sentences, verb_tags=['VBZ', 'VBP', 'VBN', 'VBG', 'VBD', 'VB']):\n",
    "    res = np.zeros((len(sentences), len(verb_tags)))\n",
    "    row = 0\n",
    "    for sentence in sentences:\n",
    "        tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        d = Counter(map(lambda tup: tup[1], filter(lambda tup: tup[1] in verb_tags, tags)))\n",
    "        for tag in verb_tags:\n",
    "            if tag not in d:\n",
    "                d[tag] = 0\n",
    "\n",
    "        verb_counts = np.array(list(map(lambda tup: tup[1], sorted(d.items(), key=lambda tup: tup[0])))) # Sort by tag, get counts\n",
    "        res[row] = verb_counts\n",
    "        row += 1\n",
    "    return res\n",
    "\n",
    "X_train_count_verbs = count_verbs(X_train)\n",
    "# count_analysis_bar(X_train_count_verbs, y_train, \"verb_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff0f8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation of feature sets into single vector:\n",
    "X_train = hstack([X_train_tfidf, X_train_tf, X_train_word2vec, X_train_count_pp, X_train_count_words, X_train_count_uppercase, X_train_flesch, X_train_count_verbs])\n",
    "X_train_filename = \"tfidf-tf-word2vec-pp-word-uppercase-flesch-verb\"\n",
    "save_npz(f\"{X_train_filename}.npz\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07fa86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features as sparse matrix\n",
    "X_train_filename = \"tfidf-tf-word2vec-pp-word-uppercase-flesch-verb\"\n",
    "X_train = load_npz(f\"{X_train_filename}.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945981a",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16011090",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler() # Sparse matrix\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f0f9a",
   "metadata": {},
   "source": [
    "### Feature selection: SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10c7dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take top 10% of features by scores given by chi2\n",
    "feature_selector = SelectPercentile(f_regression) # TODO try with mutual_info_classif instead of f_regression\n",
    "X_train = feature_selector.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce5211",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a7b26eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=SEED)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a609fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADASYN\n",
    "# ada = ADASYN(random_state=SEED)\n",
    "# X_train, y_train = ada.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f4eaf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83844d2d",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f2107",
   "metadata": {},
   "source": [
    "### Naive Bayes Model [MultinomialNB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c952f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MultinomialNB().fit(X_train, y_train) # fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89589e",
   "metadata": {},
   "source": [
    "### Logistic Regression Model [LogisticRegression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4879294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asonata/anaconda3/envs/cs4248-project/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'saga', max_iter=1000).fit(X_train, y_train) # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9bd3c",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4ae2fd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m X_val_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[1;32m      6\u001b[0m X_val_tf \u001b[38;5;241m=\u001b[39m tf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[0;32m----> 7\u001b[0m X_val_word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mword2vec_select_k_best_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_tfidf_word2vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_zero_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m X_val_count_pp \u001b[38;5;241m=\u001b[39m X_val\u001b[38;5;241m.\u001b[39mapply(count_personal_pronouns)\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m X_val_count_words \u001b[38;5;241m=\u001b[39m X_val_unprocessed\u001b[38;5;241m.\u001b[39mapply(count_words)\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[83], line 17\u001b[0m, in \u001b[0;36mword2vec_select_k_best_sum\u001b[0;34m(matrix, non_zero_counts)\u001b[0m\n\u001b[1;32m     14\u001b[0m indexes \u001b[38;5;241m=\u001b[39m row_val_idxs[top_k_idx]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get words\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer_word2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[indexes]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get summation of word vectors\u001b[39;00m\n\u001b[1;32m     20\u001b[0m summation_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39marray([spacy_model(word)\u001b[38;5;241m.\u001b[39mvector \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs4248-project/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1487\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \n\u001b[1;32m   1475\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m-> 1487\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[1;32m   1488\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[1;32m   1489\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply feature engineering on X_val\n",
    "val_tfidf_word2vec = tfidf_vectorizer_word2vec.transform(X_val)\n",
    "non_zero_counts = val_tfidf_word2vec.getnnz(axis=1)\n",
    "\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_val_tf = tf_vectorizer.transform(X_val)\n",
    "X_val_word2vec = word2vec_select_k_best_sum(val_tfidf_word2vec, non_zero_counts)\n",
    "X_val_count_pp = X_val.apply(count_personal_pronouns).to_numpy().reshape(-1, 1)\n",
    "X_val_count_words = X_val_unprocessed.apply(count_words).to_numpy().reshape(-1, 1)\n",
    "X_val_count_uppercase = X_val_unprocessed.apply(count_uppercase).to_numpy().reshape(-1, 1)\n",
    "X_val_flesch = X_val_unprocessed.apply(flesch_kincaid).to_numpy().reshape(-1, 1)\n",
    "X_val_count_verbs = count_verbs(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation of feature sets:\n",
    "X_val_final = hstack([X_val_tfidf, X_val_tf, X_val_word2vec, X_val_count_pp, X_val_count_words, X_val_count_uppercase, X_val_flesch, X_val_count_verbs])\n",
    "X_val_final = scaler.transform(X_val_final)\n",
    "X_val_final = feature_selector.transform(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0b24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6320210170765249"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain predictions on validation data\n",
    "y_val_predicted = model.predict(X_val_final)\n",
    "\n",
    "# evaluate model training metrics with macro f1 score\n",
    "f1_score(y_val, y_val_predicted, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8997e6",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47293816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "test_df = pd.read_csv('raw_data/balancedtest.csv', index_col = False)\n",
    "test_df = test_df if enable_all_data else test_df.sample(PARTITION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd783554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.iloc[:, 1]\n",
    "y_test = test_df.iloc[:, 0]\n",
    "\n",
    "# print(X_test.head())\n",
    "# print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bcfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data to match steps on training data\n",
    "X_test_unprocessed = X_test\n",
    "X_test_ls = X_test.apply(preprocess)\n",
    "X_test_sentence = X_test_ls.apply(lambda sentence: ' '.join(sentence))\n",
    "\n",
    "X_test = X_test_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863f80a",
   "metadata": {},
   "source": [
    "### Feature Engineering (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0afa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering on X_test\n",
    "test_tfidf_word2vec = tfidf_vectorizer_word2vec.transform(X_test)\n",
    "non_zero_counts = test_tfidf_word2vec.getnnz(axis=1)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_test_tf = tf_vectorizer.transform(X_test)\n",
    "X_test_word2vec = word2vec_select_k_best_sum(test_tfidf_word2vec, non_zero_counts)\n",
    "X_test_count_pp = X_test.apply(count_personal_pronouns).to_numpy().reshape(-1, 1)\n",
    "X_test_count_words = X_test_unprocessed.apply(count_words).to_numpy().reshape(-1, 1)\n",
    "X_test_count_uppercase = X_test_unprocessed.apply(count_uppercase).to_numpy().reshape(-1, 1)\n",
    "X_test_flesch = X_test_unprocessed.apply(flesch_kincaid).to_numpy().reshape(-1, 1)\n",
    "X_test_count_verbs = count_verbs(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation of feature transformations into single vector\n",
    "X_test_final = hstack([X_test_tfidf, X_test_tf, X_test_word2vec, X_test_count_pp, X_test_count_words, X_test_count_uppercase, X_test_flesch, X_test_count_verbs])\n",
    "X_test_final = scaler.transform(X_test_final)\n",
    "X_test_final = feature_selector.transform(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9170c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4723953270193005"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain predictions on test data\n",
    "y_pred = model.predict(X_test_final)\n",
    "\n",
    "# evaluate model training metrics with macro f1 score\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b2f99",
   "metadata": {},
   "source": [
    "### Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544f381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6512006559989986"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do prediction on training data\n",
    "y_train_predicted = model.predict(X_train)\n",
    "\n",
    "# obtain training f1 score\n",
    "f1_score(y_train, y_train_predicted, average='macro') # TODO this tests the model on its already trained set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ec894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# obtain test f1 score\n",
    "f1_score(y_test, y_pred, average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper Parameter tuning with GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ff02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jian Hui end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <Group Member's name> start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group member's code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d8f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843a535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e911876",
   "metadata": {},
   "outputs": [],
   "source": [
    "### <Group Member's name> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee8d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba8cfd39",
   "metadata": {},
   "source": [
    "# DEPRECATED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f121ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Sentiment analysis\n",
    "# def sentiment_analysis(sentence):\n",
    "#     doc = spacy_model(sentence)\n",
    "#     return doc._.blob.polarity\n",
    "\n",
    "# X_train_sent_ana = X_train_unprocessed.apply(sentiment_analysis).to_numpy().reshape(-1, 1)\n",
    "\n",
    "# ## Analysis of sentiment feature\n",
    "# count_analysis(X_train_sent_ana, y_train)\n",
    "\n",
    "# # Save sentiment analysis output\n",
    "# pd_sent_ana = pd.DataFrame(X_train_sent_ana, columns=[\"Sentiment\"])\n",
    "# compression_opts = dict(method='zip', archive_name='sent_ana.csv')\n",
    "# pd_sent_ana.to_csv('features/sent_ana.zip', index=False, compression=compression_opts)\n",
    "\n",
    "# 3) Subjectivity analysis \n",
    "# def subjectivity_analysis(sentence):\n",
    "#     doc = spacy_model(sentence)\n",
    "#     return doc._.blob.subjectivity\n",
    "\n",
    "# X_train_subj_ana = X_train_unprocessed.apply(subjectivity_analysis).to_numpy().reshape(-1, 1)\n",
    "\n",
    "# ## Analysis of subjectivity feature\n",
    "# count_analysis(X_train_subj_ana, y_train)\n",
    "\n",
    "# # Save sentiment analysis output\n",
    "# pd_subj_ana = pd.DataFrame(X_train_subj_ana, columns=[\"Subjectivity\"])\n",
    "# compression_opts = dict(method='zip', archive_name='subj_ana.csv')\n",
    "# pd_subj_ana.to_csv('features/subj_ana.zip', index=False, compression=compression_opts)\n",
    "\n",
    "# 4) Number of exclamation and question marks (pre-analyze first)\n",
    "# def count_exclamation_marks(sentence):\n",
    "#     return len(re.findall(r'\\!', sentence))\n",
    "\n",
    "# def count_question_marks(sentence):\n",
    "#     return len(re.findall(r'\\?', sentence))\n",
    "\n",
    "# X_train_count_ex = X_train.apply(count_exclamation_marks).to_numpy().reshape(-1, 1)\n",
    "# X_train_count_qn = X_train.apply(count_question_marks).to_numpy().reshape(-1, 1)\n",
    "\n",
    "# count_analysis(X_train_count_ex, y_train, \"Exclamation mark\")\n",
    "# count_analysis(X_train_count_qn, y_train, \"Question mark\")\n",
    "\n",
    "# 5) Count number of sentences\n",
    "# def count_sentences(sentences):\n",
    "#     return len(nltk.sent_tokenize(sentences)) \n",
    "\n",
    "# X_train_count_sentences = X_train.apply(count_sentences).to_numpy().reshape(-1, 1)\n",
    "# count_analysis_scatter(X_train_count_sentences, y_train, \"sentence_count\")\n",
    "\n",
    "# 7) text2emotion (takes too around 6 hours)\n",
    "# def emotion_vector(sentences):\n",
    "#     res = np.zeros((len(sentences), 5)) # Angry, Fear, Happy, Sad, Surprise\n",
    "#     row = 0\n",
    "#     for sentence in sentences:\n",
    "#         d = te.get_emotion(sentence)\n",
    "#         emotion_sorted = sorted(d.items(), key=lambda tup: tup[0])\n",
    "#         values = np.fromiter(map(lambda tup: tup[1], emotion_sorted), dtype=float)\n",
    "#         res[row] = values\n",
    "#         row += 1\n",
    "#     return res\n",
    "\n",
    "# X_train_emotion = emotion_vector(X_train_unprocessed).to_numpy().reshape(-1, 1)\n",
    "# count_analysis_scatter(X_train_emotion[:, 0], y_train, \"emotion_angry\")\n",
    "# count_analysis_scatter(X_train_emotion[:, 1], y_train, \"emotion_fear\")\n",
    "# count_analysis_scatter(X_train_emotion[:, 2], y_train, \"emotion_happy\")\n",
    "# count_analysis_scatter(X_train_emotion[:, 3], y_train, \"emotion_sad\")\n",
    "# count_analysis_scatter(X_train_emotion[:, 4], y_train, \"emotion_surprise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510d5573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6dc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55507397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09d284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aab69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb89dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87c63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596eb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0c61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
