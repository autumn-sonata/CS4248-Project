{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8029981,"sourceType":"datasetVersion","datasetId":4732886}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer\nfrom typing import Optional","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:35:47.992497Z","iopub.execute_input":"2024-04-05T01:35:47.992858Z","iopub.status.idle":"2024-04-05T01:35:56.653862Z","shell.execute_reply.started":"2024-04-05T01:35:47.992829Z","shell.execute_reply":"2024-04-05T01:35:56.652991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.cuda.is_available())\ntorch.cuda.device(0)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:35:56.655420Z","iopub.execute_input":"2024-04-05T01:35:56.655877Z","iopub.status.idle":"2024-04-05T01:35:56.692835Z","shell.execute_reply.started":"2024-04-05T01:35:56.655850Z","shell.execute_reply":"2024-04-05T01:35:56.691717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42 # seed random state for comparison, testing\nPARTITION_SIZE = 500 # Adjust lower if potato PC and higher if gaming rig or want results closer to actual\nenable_all_data = True # SET TO FALSE IF PREPROCESSING TAKES A LONG TIME (True = test on PARTITION_SIZE training and PARTITION_SIZE testing samples)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:35:56.693887Z","iopub.execute_input":"2024-04-05T01:35:56.694151Z","iopub.status.idle":"2024-04-05T01:35:56.704693Z","shell.execute_reply.started":"2024-04-05T01:35:56.694129Z","shell.execute_reply":"2024-04-05T01:35:56.703828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/labeled-unreliable-news-lun/fulltrain.csv', header=None, index_col = False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:35:56.706887Z","iopub.execute_input":"2024-04-05T01:35:56.707211Z","iopub.status.idle":"2024-04-05T01:35:59.964131Z","shell.execute_reply.started":"2024-04-05T01:35:56.707186Z","shell.execute_reply":"2024-04-05T01:35:59.963127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df if enable_all_data else df.sample(n=PARTITION_SIZE, random_state=SEED)\n\nX_train = df.iloc[:, 1] \ny_train = df.iloc[:, 0]\n\n# print(X_train)\n# print(y_train)\n\n# print(len(X_train))\n# print(len(y_train))\n\n# y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:35:59.965316Z","iopub.execute_input":"2024-04-05T01:35:59.965608Z","iopub.status.idle":"2024-04-05T01:35:59.982981Z","shell.execute_reply.started":"2024-04-05T01:35:59.965584Z","shell.execute_reply":"2024-04-05T01:35:59.982002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train-Validation Split","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train.values, y_train.values, test_size=0.2, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:35:59.984230Z","iopub.execute_input":"2024-04-05T01:35:59.984636Z","iopub.status.idle":"2024-04-05T01:35:59.997733Z","shell.execute_reply.started":"2024-04-05T01:35:59.984603Z","shell.execute_reply":"2024-04-05T01:35:59.996790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### Oversampling","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Architecture","metadata":{}},{"cell_type":"markdown","source":"#### Modules","metadata":{}},{"cell_type":"code","source":"class WordEmbeddings(nn.Module):\n    \"\"\"\n    Adapted from: \n    - (Embedding layer) https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py\n    - (DistilBertConfig) https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/models/distilbert/configuration_distilbert.py#L45\n    \"\"\"\n\n    def __init__(self,\n                 vocab_size=30522,\n                 max_position_embeddings=1024,\n                 dim=768,\n                 dropout=0.1,\n                 pad_token_id=0):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, dim, padding_idx=pad_token_id)\n        self.position_embeddings = nn.Embedding(max_position_embeddings, dim)\n\n        self.LayerNorm = nn.LayerNorm(dim, eps=1e-12)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"position_ids\", torch.arange(max_position_embeddings).expand((1, -1)), persistent=False\n        )\n            \n    def forward(self, input_ids: torch.Tensor, input_embeds: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Parameters:\n            input_ids (torch.Tensor):\n                torch.tensor(bs, max_seq_length) The token ids to embed.\n            input_embeds (*optional*, torch.Tensor):\n                The pre-computed word embeddings. Can only be passed if the input ids are `None`.\n\n        Returns: torch.tensor(bs, max_seq_length, dim) The embedded tokens (plus position embeddings, no token_type\n        embeddings)\n        \"\"\"\n        if input_ids is not None:\n            input_embeds = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n\n        seq_length = input_embeds.size(1)\n\n        # Setting the position-ids to the registered buffer in constructor, it helps\n        # when tracing the model without passing position-ids, solves\n        # if hasattr(self, \"position_ids\"):\n        #     position_ids = self.position_ids[:, :seq_length]\n        # else:\n        if hasattr(self, \"position_ids\"):\n            position_ids = self.position_ids[:, :seq_length]\n        else:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)  # (max_seq_length)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)  # (bs, max_seq_length)\n\n        position_embeddings = self.position_embeddings(position_ids)  # (bs, max_seq_length, dim)\n\n        embeddings = input_embeds + position_embeddings  # (bs, max_seq_length, dim)\n        embeddings = self.LayerNorm(embeddings)  # (bs, max_seq_length, dim)\n        embeddings = self.dropout(embeddings)  # (bs, max_seq_length, dim)\n        return embeddings\n\nclass Attention(nn.Module):\n    ### Implements Scaled Dot Product Attention\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, Q, K, V, mask=None, dropout=None):\n        # All inputshapes: (batch_size B, seq_len L, model_size D)\n\n        # Perform Q*K^T (* is the dot product here)\n        # We have to use torch.matmul since we work with batches!\n        out = torch.matmul(Q, K.transpose(1, 2)) # => shape (B, L, D)\n\n        # scale alignment scores\n        out = out / (Q.shape[-1] **0.5)\n\n        # Push through softmax layer\n        out = nn.functional.softmax(out, dim=-1)\n\n        # Multiply scaled alignment scores with values V\n        return torch.matmul(out, V)\n\nclass AttentionHead(nn.Module):\n    def __init__(self, model_size, qkv_size):\n        super().__init__()\n        self.Wq = nn.Linear(model_size, qkv_size)\n        self.Wk = nn.Linear(model_size, qkv_size)\n        self.Wv = nn.Linear(model_size, qkv_size)\n        self.attention = Attention()\n\n    def forward(self, queries, keys, values):\n        # Computes scaled dot-product attention\n        return self.attention(self.Wq(queries),\n                              self.Wk(keys),\n                              self.Wv(values))\n \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, model_size, qkv_size):\n        super().__init__()\n\n        # Define num_heads attention heads\n        self.heads = nn.ModuleList(\n            [ AttentionHead(model_size, qkv_size) for _ in range(num_heads) ]\n        )\n\n        # Linear layer to \"unify\" all heads into one\n        self.Wo = nn.Linear(num_heads * qkv_size, model_size)\n\n    def forward(self, query, key, value):\n        # Compute the outputs for all attention heads\n        out_heads = [ head(query, key, value) for head in self.heads ]\n\n        # Concatenate output of all attention heads\n        out = torch.cat(out_heads, dim=-1)\n\n        # Unify concatenated output to the model size\n        return self.Wo(out)\n\nclass FeedForward(nn.Module):\n    def __init__(self, model_size, hidden_size=2048):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(model_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, model_size),\n        )\n    \n    def forward(self, X):\n        return self.net(X)\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, model_size, num_heads, ff_hidden_size, dropout):\n        super().__init__()\n\n        # Define sizes of Q/K/V based on model size and number of heads\n        qkv_size = max(model_size // num_heads, 1)\n\n        # MultiHeadAttention block\n        self.mhal = MultiHeadAttention(num_heads, model_size, qkv_size)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(model_size)\n\n        # FeedForward block\n        self.ff = FeedForward(model_size, ff_hidden_size)\n        self.dropout2 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(model_size)\n\n    def forward(self, source):\n        # MultiHeadAttention block\n        out1 = self.mhal(source, source, source)\n        out1 = self.dropout1(out1)\n        out1 = self.norm1(out1 + source)\n\n        # FeedForward block\n        out2 = self.ff(out1)\n        out2 = self.dropout2(out2)\n        out2 = self.norm2(out2)\n\n        return out2\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self,\n                 num_layers=6,\n                 model_size=768,\n                 num_heads=8,\n                 ff_hidden_size=2048,\n                 dropout=0.1):\n        super().__init__()\n\n        #define num_layers (N) encoder layers\n        self.layers = nn.ModuleList(\n            [ TransformerEncoderLayer(model_size,\n                                    num_heads,\n                                    ff_hidden_size,\n                                    dropout)\n              for _ in range(num_layers)\n            ]\n        )\n    \n    def forward(self, source):\n        # Push through each encoder layer\n        for l in self.layers:\n            source = l(source)\n        return source\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:35:59.999225Z","iopub.execute_input":"2024-04-05T01:35:59.999506Z","iopub.status.idle":"2024-04-05T01:36:00.028676Z","shell.execute_reply.started":"2024-04-05T01:35:59.999483Z","shell.execute_reply":"2024-04-05T01:36:00.027537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Classifier","metadata":{}},{"cell_type":"code","source":"class EncoderOnlyClassificationModel(nn.Module):\n    \"\"\"\n    References: \n    - (DistilBertForSequenceClassification) https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py\n    - (DistilBertConfig) https://github.com/huggingface/transformers/blob/v4.39.3/src/transformers/models/distilbert/configuration_distilbert.py#L45\n    \"\"\"\n    def __init__(self,\n                 num_layers=6,\n                 model_size=768,\n                 num_heads=8,\n                 ff_hidden_size=2048,\n                 encoder_dropout=0.1,\n                 classifier_dropout=0.2,\n                 num_classes=4):\n        super().__init__()\n\n        self.embeddings = WordEmbeddings(dim=model_size)\n        self.encoder = TransformerEncoder(num_layers, model_size, num_heads, ff_hidden_size, encoder_dropout)\n        self.pre_classifier = nn.Linear(model_size, model_size)\n        self.classifier = nn.Linear(model_size, num_classes)\n        self.dropout = nn.Dropout(classifier_dropout)\n    \n    def forward(self, input_ids, attention_mask):\n        embeddings = self.embeddings(input_ids, attention_mask)\n        output = self.encoder(embeddings)[:, 0]\n        output = self.pre_classifier(output)\n        output = nn.ReLU()(output)\n        output = self.dropout(output)\n        output = self.classifier(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:36:00.030008Z","iopub.execute_input":"2024-04-05T01:36:00.030322Z","iopub.status.idle":"2024-04-05T01:36:00.043345Z","shell.execute_reply.started":"2024-04-05T01:36:00.030297Z","shell.execute_reply":"2024-04-05T01:36:00.042399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y, tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')):\n        self.X = X\n        self.y = y\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer.encode_plus(\n            str(self.X[idx]),\n            add_special_tokens=True,\n            max_length=1024,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        y_tensor = torch.tensor(self.y[idx]).long()\n        return encoding['input_ids'].flatten().to(device), encoding['attention_mask'].flatten().to(device), y_tensor.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:36:00.044555Z","iopub.execute_input":"2024-04-05T01:36:00.044915Z","iopub.status.idle":"2024-04-05T01:36:01.574125Z","shell.execute_reply.started":"2024-04-05T01:36:00.044886Z","shell.execute_reply":"2024-04-05T01:36:01.572966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EncoderOnlyClassificationModel().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T01:36:01.578138Z","iopub.execute_input":"2024-04-05T01:36:01.578456Z","iopub.status.idle":"2024-04-05T01:36:02.498813Z","shell.execute_reply.started":"2024-04-05T01:36:01.578428Z","shell.execute_reply":"2024-04-05T01:36:02.497968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(X_train, y_train)\nval_dataset = CustomDataset(X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:18:57.844723Z","iopub.execute_input":"2024-04-05T08:18:57.845066Z","iopub.status.idle":"2024-04-05T08:18:58.139972Z","shell.execute_reply.started":"2024-04-05T08:18:57.845036Z","shell.execute_reply":"2024-04-05T08:18:58.138517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\n\noptimiser = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n\ntorch.manual_seed(SEED)\n\nEPOCHS = 2\n\nfor epoch in range(EPOCHS):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n\n    for batch_idx, (input_ids, attention_mask, labels) in enumerate(train_loader):\n        optimiser.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = loss_fn(outputs, labels - 1)\n        loss.backward()\n        optimiser.step()\n\n        total_loss += loss.item()\n        \n    # obtain predictions on val data\n    model.eval()\n    y_pred_val = []\n    y_true_val = []\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n\n    with torch.no_grad():\n        for batch_idx, (input_ids, attention_mask, labels) in enumerate(val_loader):\n            outputs = model(input_ids, attention_mask).cpu()\n            y_pred_val.extend(torch.argmax(outputs, dim=1).numpy())\n            y_true_val.extend(labels.cpu().numpy())\n\n    val_accuracy = accuracy_score(y_true_val - 1, y_pred_val)\n    val_f1 = f1_score(y_true_val - 1, y_pred_val, average='macro')\n    # Print average loss for the epoch\n    print(f'Epoch {epoch + 1}/{EPOCHS},\\nLoss: {total_loss / len(loader)},\\nValidation Accuracy: {val_accuracy:.4f},\\nValidation f1: {val_f1:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:24:33.124495Z","iopub.execute_input":"2024-04-05T08:24:33.124912Z","iopub.status.idle":"2024-04-05T08:24:33.487314Z","shell.execute_reply.started":"2024-04-05T08:24:33.124880Z","shell.execute_reply":"2024-04-05T08:24:33.484239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save model","metadata":{}},{"cell_type":"code","source":"checkpoint = {'model': EncoderOnlyClassificationModel(),\n              'state_dict': model.state_dict(),\n              'optimiser' : optimiser.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T03:07:45.534263Z","iopub.execute_input":"2024-04-05T03:07:45.535186Z","iopub.status.idle":"2024-04-05T03:07:47.584502Z","shell.execute_reply.started":"2024-04-05T03:07:45.535153Z","shell.execute_reply":"2024-04-05T03:07:47.583721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Data","metadata":{}},{"cell_type":"code","source":"# TEST DATA \ntest_df = pd.read_csv('/kaggle/input/labeled-unreliable-news-lun/balancedtest.csv', index_col = False)\ntest_df = test_df if enable_all_data else test_df.sample(PARTITION_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T03:13:50.069844Z","iopub.execute_input":"2024-04-05T03:13:50.070100Z","iopub.status.idle":"2024-04-05T03:13:50.246435Z","shell.execute_reply.started":"2024-04-05T03:13:50.070079Z","shell.execute_reply":"2024-04-05T03:13:50.245696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test_df.iloc[:, 1]\ny_test = test_df.iloc[:, 0]","metadata":{"execution":{"iopub.status.busy":"2024-04-05T03:13:50.247497Z","iopub.execute_input":"2024-04-05T03:13:50.247810Z","iopub.status.idle":"2024-04-05T03:13:50.252726Z","shell.execute_reply.started":"2024-04-05T03:13:50.247784Z","shell.execute_reply":"2024-04-05T03:13:50.251742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Engineering (Test Data)","metadata":{}},{"cell_type":"code","source":"test_dataset = CustomDataset(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T03:13:50.253977Z","iopub.execute_input":"2024-04-05T03:13:50.254234Z","iopub.status.idle":"2024-04-05T03:13:50.262284Z","shell.execute_reply.started":"2024-04-05T03:13:50.254213Z","shell.execute_reply":"2024-04-05T03:13:50.261552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain predictions on test data\nmodel.eval()\n\ny_pred = []\ny_true_test = []\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n\nwith torch.no_grad():\n    for batch_idx, (input_ids, attention_mask, labels) in enumerate(test_loader):\n        outputs = model(input_ids, attention_mask).cpu()\n        y_pred.extend(torch.argmax(outputs, dim=1).numpy())\n        y_true_test.extend(labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-04-05T03:13:50.263266Z","iopub.execute_input":"2024-04-05T03:13:50.263541Z","iopub.status.idle":"2024-04-05T03:15:37.637707Z","shell.execute_reply.started":"2024-04-05T03:13:50.263518Z","shell.execute_reply":"2024-04-05T03:15:37.636939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_true_test - 1, y_pred)\ntest_f1 = f1_score(y_true_test - 1, y_pred, average='macro')\n\nprint(f'Test Accuracy: {test_accuracy:.4f},\\nTest f1: {test_f1:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T08:30:28.875732Z","iopub.execute_input":"2024-04-05T08:30:28.876294Z","iopub.status.idle":"2024-04-05T08:30:28.904343Z","shell.execute_reply.started":"2024-04-05T08:30:28.876258Z","shell.execute_reply":"2024-04-05T08:30:28.902843Z"},"trusted":true},"execution_count":null,"outputs":[]}]}